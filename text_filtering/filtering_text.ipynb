{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.58k/1.58k [00:00<00:00, 1.62MB/s]\n",
      "Using custom data configuration smilegate-ai--kor_unsmile-1dba960877497f9f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 1.39 MiB, generated: 4.93 MiB, post-processed: Unknown size, total: 6.32 MiB) to C:\\Users\\HP\\.cache\\huggingface\\datasets\\parquet\\smilegate-ai--kor_unsmile-1dba960877497f9f\\0.0.0\\1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.16M/1.16M [00:00<00:00, 1.19MB/s]\n",
      "Downloading: 100%|██████████| 290k/290k [00:00<00:00, 41.5MB/s]\n",
      "100%|██████████| 2/2 [00:06<00:00,  3.23s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2054.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to C:\\Users\\HP\\.cache\\huggingface\\datasets\\parquet\\smilegate-ai--kor_unsmile-1dba960877497f9f\\0.0.0\\1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 76.51it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('smilegate-ai/kor_unsmile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'문장': '일안하는 시간은 쉬고싶어서 그런게 아닐까',\n",
       " '여성/가족': 0,\n",
       " '남성': 0,\n",
       " '성소수자': 0,\n",
       " '인종/국적': 0,\n",
       " '연령': 0,\n",
       " '지역': 0,\n",
       " '종교': 0,\n",
       " '기타 혐오': 0,\n",
       " '악플/욕설': 0,\n",
       " 'clean': 1,\n",
       " '개인지칭': 0,\n",
       " 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsmile_labels = [\"여성/가족\",\"남성\",\"성소수자\",\"인종/국적\",\"연령\",\"지역\",\"종교\",\"기타 혐오\",\"악플/욕설\",\"clean\"]\n",
    "# 개인지칭의 경우, 추가 정보이므로 분류 대상에서 제외했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1175 [11:46<11:56:32, 37.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kcbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"\\uc5ec\\uc131/\\uac00\\uc871\",\n",
      "    \"1\": \"\\ub0a8\\uc131\",\n",
      "    \"2\": \"\\uc131\\uc18c\\uc218\\uc790\",\n",
      "    \"3\": \"\\uc778\\uc885/\\uad6d\\uc801\",\n",
      "    \"4\": \"\\uc5f0\\ub839\",\n",
      "    \"5\": \"\\uc9c0\\uc5ed\",\n",
      "    \"6\": \"\\uc885\\uad50\",\n",
      "    \"7\": \"\\uae30\\ud0c0 \\ud610\\uc624\",\n",
      "    \"8\": \"\\uc545\\ud50c/\\uc695\\uc124\",\n",
      "    \"9\": \"clean\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"clean\": 9,\n",
      "    \"\\uae30\\ud0c0 \\ud610\\uc624\": 7,\n",
      "    \"\\ub0a8\\uc131\": 1,\n",
      "    \"\\uc131\\uc18c\\uc218\\uc790\": 2,\n",
      "    \"\\uc545\\ud50c/\\uc695\\uc124\": 8,\n",
      "    \"\\uc5ec\\uc131/\\uac00\\uc871\": 0,\n",
      "    \"\\uc5f0\\ub839\": 4,\n",
      "    \"\\uc778\\uc885/\\uad6d\\uc801\": 3,\n",
      "    \"\\uc885\\uad50\": 6,\n",
      "    \"\\uc9c0\\uc5ed\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 300,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at smilegate-ai/kor_unsmile.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\HP/.cache\\huggingface\\hub\\models--smilegate-ai--kor_unsmile\\snapshots\\f597423eeff8e6da99cad85cbe3a81adf5225637\\tokenizer_config.json\n",
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:92: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline, BertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = 'smilegate-ai/kor_unsmile'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0,     # cpu: -1, gpu: gpu number\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '여성/가족', 'score': 0.8253052234649658}\n",
      "{'label': '남성', 'score': 0.039725154638290405}\n",
      "{'label': '성소수자', 'score': 0.01214433740824461}\n",
      "{'label': '인종/국적', 'score': 0.023181872442364693}\n",
      "{'label': '연령', 'score': 0.010315308347344398}\n",
      "{'label': '지역', 'score': 0.018454890698194504}\n",
      "{'label': '종교', 'score': 0.011270025745034218}\n",
      "{'label': '기타 혐오', 'score': 0.0207340307533741}\n",
      "{'label': '악플/욕설', 'score': 0.057331446558237076}\n",
      "{'label': 'clean', 'score': 0.14010532200336456}\n"
     ]
    }
   ],
   "source": [
    "for result in pipe(\"이래서 여자는 게임을 하면 안된다\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\transformers\\pipelines\\base.py:1046: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pipe(\"게임을 뭐 이따구로 만드냐 진짜 할맛 안난다\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\transformers\\pipelines\\base.py:1046: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14892\\2162267316.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"이래서 여자는 게임을 하면 안된다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"이래서 여자는 게임을 하면 안된다\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'dict' and 'dict'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n",
      "100%|██████████| 3737/3737 [00:26<00:00, 140.66it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from transformers.pipelines.base import KeyDataset\n",
    "\n",
    "predicated_labels = []\n",
    "\n",
    "for out in tqdm.tqdm(pipe(KeyDataset(dataset['valid'], '문장'))):\n",
    "    predicated_labels.append(get_predicated_label(out, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.70      0.76       394\n",
      "           1       0.87      0.83      0.85       334\n",
      "           2       0.90      0.78      0.83       280\n",
      "           3       0.87      0.79      0.82       426\n",
      "           4       0.92      0.75      0.83       146\n",
      "           5       0.87      0.88      0.88       260\n",
      "           6       0.87      0.86      0.87       290\n",
      "           7       0.92      0.18      0.30       134\n",
      "           8       0.76      0.59      0.67       786\n",
      "           9       0.74      0.79      0.77       935\n",
      "\n",
      "   micro avg       0.82      0.73      0.77      3985\n",
      "   macro avg       0.86      0.72      0.76      3985\n",
      "weighted avg       0.82      0.73      0.77      3985\n",
      " samples avg       0.76      0.74      0.75      3985\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(dataset['valid']['labels'], predicated_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration smilegate-ai--kor_unsmile-1dba960877497f9f\n",
      "Reusing dataset parquet (C:\\Users\\HP\\.cache\\huggingface\\datasets\\parquet\\smilegate-ai--kor_unsmile-1dba960877497f9f\\0.0.0\\1638526fd0e8d960534e2155dc54fdff8dce73851f21f031d2fb9c2cf757c121)\n",
      "100%|██████████| 2/2 [00:00<00:00, 683.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('smilegate-ai/kor_unsmile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 49.0/49.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 619/619 [00:00<00:00, 636kB/s]\n",
      "Downloading: 100%|██████████| 250k/250k [00:00<00:00, 263kB/s] \n"
     ]
    }
   ],
   "source": [
    "model_name = 'beomi/kcbert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    tokenized_examples = tokenizer(str(examples[\"문장\"]))\n",
    "    tokenized_examples['labels'] = torch.tensor(examples[\"labels\"], dtype=torch.float)\n",
    "    # multi label classification 학습을 위해선 label이 float 형태로 변형되어야 합니다.\n",
    "    # huggingface datasets 최신 버전에는 'map' 함수에 버그가 있어서 변형이 올바르게 되지 않습니다.\n",
    "    \n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x0000015F54EC4B88> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 15005/15005 [00:02<00:00, 5229.28ex/s]\n",
      "100%|██████████| 3737/3737 [00:00<00:00, 5355.60ex/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask', 'token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " 'input_ids': tensor([    2,  2458, 15751, 24930, 24351, 29278, 17038, 11631,     3]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 438M/438M [00:05<00:00, 86.9MB/s] \n",
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels=len(unsmile_labels) # Label 갯수\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model.config.id2label = {i: label for i, label in zip(range(num_labels), unsmile_labels)}\n",
    "model.config.label2id = {label: i for i, label in zip(range(num_labels), unsmile_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'여성/가족': 0,\n",
       " '남성': 1,\n",
       " '성소수자': 2,\n",
       " '인종/국적': 3,\n",
       " '연령': 4,\n",
       " '지역': 5,\n",
       " '종교': 6,\n",
       " '기타 혐오': 7,\n",
       " '악플/욕설': 8,\n",
       " 'clean': 9}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(x):\n",
    "    return {\n",
    "        'lrap': label_ranking_average_precision_score(x.label_ids, x.predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # 64 batch는 colab pro에서 테스트되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/model/unsmile\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='lrap',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=tokenized_dataset[\"train\"], \n",
    "    eval_dataset=tokenized_dataset[\"valid\"], \n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\final\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:92: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': '여성/가족', 'score': 0.15060104429721832}\n",
      "{'label': '남성', 'score': 0.18680186569690704}\n",
      "{'label': '성소수자', 'score': 0.11889719218015671}\n",
      "{'label': '인종/국적', 'score': 0.153749018907547}\n",
      "{'label': '연령', 'score': 0.10417928546667099}\n",
      "{'label': '지역', 'score': 0.11103798449039459}\n",
      "{'label': '종교', 'score': 0.16230972111225128}\n",
      "{'label': '기타 혐오', 'score': 0.10309185087680817}\n",
      "{'label': '악플/욕설', 'score': 0.31387609243392944}\n",
      "{'label': 'clean', 'score': 0.2767322063446045}\n"
     ]
    }
   ],
   "source": [
    "for result in pipe(\"이래서 여자는 게임을 하면 안된다\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicated_label(output_labels, min_score):\n",
    "    labels = []\n",
    "    for label in output_labels:\n",
    "        if label['score'] > min_score:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "pipe = TextClassificationPipeline(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    device=0,\n",
    "    return_all_scores=True,\n",
    "    function_to_apply='sigmoid'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in pipe(\"이래서 여자는 게임을 하면 안된다\")[0]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('final')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c5d292433a8ddcc1675e1c3b36ec13bf3f1c003e8eead3bc3fefbe0799906b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
