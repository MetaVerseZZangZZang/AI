{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hds1ZO4d8_Am"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def scan_vocabulary(sents, tokenize=None, min_count=2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(str) returns list of str\n",
        "    min_count : int\n",
        "        Minumum term frequency\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    idx_to_vocab : list of str\n",
        "        Vocabulary list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "    \"\"\"\n",
        "    counter = Counter(w for sent in sents for w in tokenize(sent))\n",
        "    counter = {w:c for w,c in counter.items() if c >= min_count}\n",
        "    idx_to_vocab = [w for w, _ in sorted(counter.items(), key=lambda x:-x[1])]\n",
        "    vocab_to_idx = {vocab:idx for idx, vocab in enumerate(idx_to_vocab)}\n",
        "    return idx_to_vocab, vocab_to_idx\n",
        "\n",
        "def tokenize_sents(sents, tokenize):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) returns list of str (word sequence)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tokenized sentence list : list of list of str\n",
        "    \"\"\"\n",
        "    return [tokenize(sent) for sent in sents]\n",
        "\n",
        "def vectorize(tokens, vocab_to_idx):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    tokens : list of list of str\n",
        "        Tokenzed sentence list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sentence bow : scipy.sparse.csr_matrix\n",
        "        shape = (n_sents, n_terms)\n",
        "    \"\"\"\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        for t, c in Counter(tokens_i).items():\n",
        "            j = vocab_to_idx.get(t, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(c)\n",
        "    n_sents = len(tokens)\n",
        "    n_terms = len(vocab_to_idx)\n",
        "    x = csr_matrix((data, (rows, cols)), shape=(n_sents, n_terms))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "#from utils import scan_vocabulary\n",
        "#from utils import tokenize_sents\n",
        "\n",
        "\n",
        "def word_graph(sents, tokenize=None, min_count=2, window=2,\n",
        "    min_cooccurrence=2, vocab_to_idx=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(str) returns list of str\n",
        "    min_count : int\n",
        "        Minumum term frequency\n",
        "    window : int\n",
        "        Co-occurrence window size\n",
        "    min_cooccurrence : int\n",
        "        Minimum cooccurrence frequency\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "        If None, this function scan vocabulary first.\n",
        "    verbose : Boolean\n",
        "        If True, verbose mode on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    co-occurrence word graph : scipy.sparse.csr_matrix\n",
        "    idx_to_vocab : list of str\n",
        "        Word list corresponding row and column\n",
        "    \"\"\"\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    tokens = tokenize_sents(sents, tokenize)\n",
        "    g = cooccurrence(tokens, vocab_to_idx, window, min_cooccurrence, verbose)\n",
        "    return g, idx_to_vocab\n",
        "\n",
        "def cooccurrence(tokens, vocab_to_idx, window=2, min_cooccurrence=2, verbose=False):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    tokens : list of list of str\n",
        "        Tokenized sentence list\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper\n",
        "    window : int\n",
        "        Co-occurrence window size\n",
        "    min_cooccurrence : int\n",
        "        Minimum cooccurrence frequency\n",
        "    verbose : Boolean\n",
        "        If True, verbose mode on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    co-occurrence matrix : scipy.sparse.csr_matrix\n",
        "        shape = (n_vocabs, n_vocabs)\n",
        "    \"\"\"\n",
        "    counter = defaultdict(int)\n",
        "    for s, tokens_i in enumerate(tokens):\n",
        "        if verbose and s % 1000 == 0:\n",
        "            print('\\rword cooccurrence counting {}'.format(s), end='')\n",
        "        vocabs = [vocab_to_idx[w] for w in tokens_i if w in vocab_to_idx]\n",
        "        n = len(vocabs)\n",
        "        for i, v in enumerate(vocabs):\n",
        "            if window <= 0:\n",
        "                b, e = 0, n\n",
        "            else:\n",
        "                b = max(0, i - window)\n",
        "                e = min(i + window, n)\n",
        "            for j in range(b, e):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                counter[(v, vocabs[j])] += 1\n",
        "                counter[(vocabs[j], v)] += 1\n",
        "    counter = {k:v for k,v in counter.items() if v >= min_cooccurrence}\n",
        "    n_vocabs = len(vocab_to_idx)\n",
        "    if verbose:\n",
        "        print('\\rword cooccurrence counting from {} sents was done'.format(s+1))\n",
        "    return dict_to_mat(counter, n_vocabs, n_vocabs)\n",
        "\n",
        "def dict_to_mat(d, n_rows, n_cols):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    d : dict\n",
        "        key : (i,j) tuple\n",
        "        value : float value\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    scipy.sparse.csr_matrix\n",
        "    \"\"\"\n",
        "    rows, cols, data = [], [], []\n",
        "    for (i, j), v in d.items():\n",
        "        rows.append(i)\n",
        "        cols.append(j)\n",
        "        data.append(v)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n"
      ],
      "metadata": {
        "id": "8Kwv69OS9EDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "#from .utils import scan_vocabulary\n",
        "#from .utils import tokenize_sents\n",
        "\n",
        "\n",
        "def sent_graph(sents, tokenize=None, min_count=2, min_sim=0.3,\n",
        "    similarity=None, vocab_to_idx=None, verbose=False):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        tokenize(sent) return list of str\n",
        "    min_count : int\n",
        "        Minimum term frequency\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences\n",
        "    similarity : callable or str\n",
        "        similarity(s1, s2) returns float\n",
        "        s1 and s2 are list of str.\n",
        "        available similarity = [callable, 'cosine', 'textrank']\n",
        "    vocab_to_idx : dict\n",
        "        Vocabulary to index mapper.\n",
        "        If None, this function scan vocabulary first.\n",
        "    verbose : Boolean\n",
        "        If True, verbose mode on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sentence similarity graph : scipy.sparse.csr_matrix\n",
        "        shape = (n sents, n sents)\n",
        "    \"\"\"\n",
        "\n",
        "    if vocab_to_idx is None:\n",
        "        idx_to_vocab, vocab_to_idx = scan_vocabulary(sents, tokenize, min_count)\n",
        "    else:\n",
        "        idx_to_vocab = [vocab for vocab, _ in sorted(vocab_to_idx.items(), key=lambda x:x[1])]\n",
        "\n",
        "    x = vectorize_sents(sents, tokenize, vocab_to_idx)\n",
        "    if similarity == 'cosine':\n",
        "        x = numpy_cosine_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    else:\n",
        "        x = numpy_textrank_similarity_matrix(x, min_sim, verbose, batch_size=1000)\n",
        "    return x\n",
        "\n",
        "def vectorize_sents(sents, tokenize, vocab_to_idx):\n",
        "    rows, cols, data = [], [], []\n",
        "    for i, sent in enumerate(sents):\n",
        "        counter = Counter(tokenize(sent))\n",
        "        for token, count in counter.items():\n",
        "            j = vocab_to_idx.get(token, -1)\n",
        "            if j == -1:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(count)\n",
        "    n_rows = len(sents)\n",
        "    n_cols = len(vocab_to_idx)\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "def numpy_cosine_similarity_matrix(x, min_sim=0.3, verbose=True, batch_size=1000):\n",
        "    n_rows = x.shape[0]\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "        psim = 1 - pairwise_distances(x[b:e], x, metric='cosine')\n",
        "        rows, cols = np.where(psim >= min_sim)\n",
        "        data = psim[rows, cols]\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "        if verbose:\n",
        "            print('\\rcalculating cosine sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating cosine sentence similarity was done with {} sents'.format(n_rows))\n",
        "    return mat\n",
        "\n",
        "def numpy_textrank_similarity_matrix(x, min_sim=0.3, verbose=True, min_length=1, batch_size=1000):\n",
        "    n_rows, n_cols = x.shape\n",
        "\n",
        "    # Boolean matrix\n",
        "    rows, cols = x.nonzero()\n",
        "    data = np.ones(rows.shape[0])\n",
        "    z = csr_matrix((data, (rows, cols)), shape=(n_rows, n_cols))\n",
        "\n",
        "    # Inverse sentence length\n",
        "    size = np.asarray(x.sum(axis=1)).reshape(-1)\n",
        "    size[np.where(size <= min_length)] = 10000\n",
        "    size = np.log(size)\n",
        "\n",
        "    mat = []\n",
        "    for bidx in range(math.ceil(n_rows / batch_size)):\n",
        "\n",
        "        # slicing\n",
        "        b = int(bidx * batch_size)\n",
        "        e = min(n_rows, int((bidx+1) * batch_size))\n",
        "\n",
        "        # dot product\n",
        "        inner = z[b:e,:] * z.transpose()\n",
        "\n",
        "        # sentence len[i,j] = size[i] + size[j]\n",
        "        norm = size[b:e].reshape(-1,1) + size.reshape(1,-1)\n",
        "        norm = norm ** (-1)\n",
        "        norm[np.where(norm == np.inf)] = 0\n",
        "\n",
        "        # normalize\n",
        "        sim = inner.multiply(norm).tocsr()\n",
        "        rows, cols = (sim >= min_sim).nonzero()\n",
        "        data = np.asarray(sim[rows, cols]).reshape(-1)\n",
        "\n",
        "        # append\n",
        "        mat.append(csr_matrix((data, (rows, cols)), shape=(e-b, n_rows)))\n",
        "\n",
        "        if verbose:\n",
        "            print('\\rcalculating textrank sentence similarity {} / {}'.format(b, n_rows), end='')\n",
        "\n",
        "    mat = sp.sparse.vstack(mat)\n",
        "    if verbose:\n",
        "        print('\\rcalculating textrank sentence similarity was done with {} sents'.format(n_rows))\n",
        "\n",
        "    return mat\n",
        "\n",
        "def graph_with_python_sim(tokens, verbose, similarity, min_sim):\n",
        "    if similarity == 'cosine':\n",
        "        similarity = cosine_sent_sim\n",
        "    elif callable(similarity):\n",
        "        similarity = similarity\n",
        "    else:\n",
        "        similarity = textrank_sent_sim\n",
        "\n",
        "    rows, cols, data = [], [], []\n",
        "    n_sents = len(tokens)\n",
        "    for i, tokens_i in enumerate(tokens):\n",
        "        if verbose and i % 1000 == 0:\n",
        "            print('\\rconstructing sentence graph {} / {} ...'.format(i, n_sents), end='')\n",
        "        for j, tokens_j in enumerate(tokens):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            sim = similarity(tokens_i, tokens_j)\n",
        "            if sim < min_sim:\n",
        "                continue\n",
        "            rows.append(i)\n",
        "            cols.append(j)\n",
        "            data.append(sim)\n",
        "    if verbose:\n",
        "        print('\\rconstructing sentence graph was constructed from {} sents'.format(n_sents))\n",
        "    return csr_matrix((data, (rows, cols)), shape=(n_sents, n_sents))\n",
        "\n",
        "def textrank_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    n1 = len(s1)\n",
        "    n2 = len(s2)\n",
        "    if (n1 <= 1) or (n2 <= 1):\n",
        "        return 0\n",
        "    common = len(set(s1).intersection(set(s2)))\n",
        "    base = math.log(n1) + math.log(n2)\n",
        "    return common / base\n",
        "\n",
        "def cosine_sent_sim(s1, s2):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    s1, s2 : list of str\n",
        "        Tokenized sentences\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Sentence similarity : float\n",
        "        Non-negative number\n",
        "    \"\"\"\n",
        "    if (not s1) or (not s2):\n",
        "        return 0\n",
        "\n",
        "    s1 = Counter(s1)\n",
        "    s2 = Counter(s2)\n",
        "    norm1 = math.sqrt(sum(v ** 2 for v in s1.values()))\n",
        "    norm2 = math.sqrt(sum(v ** 2 for v in s2.values()))\n",
        "    prod = 0\n",
        "    for k, v in s1.items():\n",
        "        prod += v * s2.get(k, 0)\n",
        "    return prod / (norm1 * norm2)\n"
      ],
      "metadata": {
        "id": "iXL0ZccI9YLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def pagerank(x, df=0.85, max_iter=30, bias=None):\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    x : scipy.sparse.csr_matrix\n",
        "        shape = (n vertex, n vertex)\n",
        "    df : float\n",
        "        Damping factor, 0 < df < 1\n",
        "    max_iter : int\n",
        "        Maximum number of iteration\n",
        "    bias : numpy.ndarray or None\n",
        "        If None, equal bias\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    R : numpy.ndarray\n",
        "        PageRank vector. shape = (n vertex, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1,1)\n",
        "\n",
        "    # check bias\n",
        "    if bias is None:\n",
        "        bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)\n",
        "    else:\n",
        "        bias = bias.reshape(-1,1)\n",
        "        bias = A.shape[0] * bias / bias.sum()\n",
        "        assert bias.shape[0] == A.shape[0]\n",
        "        bias = (1 - df) * bias\n",
        "\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias\n",
        "\n",
        "    return R\n"
      ],
      "metadata": {
        "id": "KpU_z5HZ9qJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from .rank import pagerank\n",
        "#from .sentence import sent_graph\n",
        "#from .word import word_graph\n",
        "\n",
        "\n",
        "class KeywordSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    window : int\n",
        "        Word cooccurrence window size. Default is -1.\n",
        "        '-1' means there is cooccurrence between two words if the words occur in a sentence\n",
        "    min_cooccurrence : int\n",
        "        Minimum cooccurrence frequency of two words\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        window=-1, min_cooccurrence=2, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.window = window\n",
        "        self.min_cooccurrence = min_cooccurrence\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "\n",
        "        g, self.idx_to_vocab = word_graph(sents,\n",
        "            self.tokenize, self.min_count,self.window,\n",
        "            self.min_cooccurrence, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n words = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def keywords(self, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'R'):\n",
        "            raise RuntimeError('Train textrank first or use summarize function')\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keywords = [(self.idx_to_vocab[idx], self.R[idx]) for idx in reversed(idxs)]\n",
        "        return keywords\n",
        "\n",
        "    def summarize(self, sents, topk=30):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of keywords selected from TextRank\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        keywords : list of tuple\n",
        "            Each tuple stands for (word, rank)\n",
        "        \"\"\"\n",
        "        self.train_textrank(sents)\n",
        "        return self.keywords(topk)\n",
        "\n",
        "\n",
        "class KeysentenceSummarizer:\n",
        "    \"\"\"\n",
        "    Arguments\n",
        "    ---------\n",
        "    sents : list of str\n",
        "        Sentence list\n",
        "    tokenize : callable\n",
        "        Tokenize function: tokenize(str) = list of str\n",
        "    min_count : int\n",
        "        Minumum frequency of words will be used to construct sentence graph\n",
        "    min_sim : float\n",
        "        Minimum similarity between sentences in sentence graph\n",
        "    similarity : str\n",
        "        available similarity = ['cosine', 'textrank']\n",
        "    vocab_to_idx : dict or None\n",
        "        Vocabulary to index mapper\n",
        "    df : float\n",
        "        PageRank damping factor\n",
        "    max_iter : int\n",
        "        Number of PageRank iterations\n",
        "    verbose : Boolean\n",
        "        If True, it shows training progress\n",
        "    \"\"\"\n",
        "    def __init__(self, sents=None, tokenize=None, min_count=2,\n",
        "        min_sim=0.3, similarity=None, vocab_to_idx=None,\n",
        "        df=0.85, max_iter=30, verbose=False):\n",
        "\n",
        "        self.tokenize = tokenize\n",
        "        self.min_count = min_count\n",
        "        self.min_sim = min_sim\n",
        "        self.similarity = similarity\n",
        "        self.vocab_to_idx = vocab_to_idx\n",
        "        self.df = df\n",
        "        self.max_iter = max_iter\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if sents is not None:\n",
        "            self.train_textrank(sents)\n",
        "\n",
        "    def train_textrank(self, sents, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        g = sent_graph(sents, self.tokenize, self.min_count,\n",
        "            self.min_sim, self.similarity, self.vocab_to_idx, self.verbose)\n",
        "        self.R = pagerank(g, self.df, self.max_iter, bias).reshape(-1)\n",
        "        if self.verbose:\n",
        "            print('trained TextRank. n sentences = {}'.format(self.R.shape[0]))\n",
        "\n",
        "    def summarize(self, sents, topk=30, bias=None):\n",
        "        \"\"\"\n",
        "        Arguments\n",
        "        ---------\n",
        "        sents : list of str\n",
        "            Sentence list\n",
        "        topk : int\n",
        "            Number of key-sentences to be selected.\n",
        "        bias : None or numpy.ndarray\n",
        "            PageRank bias term\n",
        "            Shape must be (n_sents,)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        keysents : list of tuple\n",
        "            Each tuple stands for (sentence index, rank, sentence)\n",
        "\n",
        "        Usage\n",
        "        -----\n",
        "            >>> from textrank import KeysentenceSummarizer\n",
        "\n",
        "            >>> summarizer = KeysentenceSummarizer(tokenize = tokenizer, min_sim = 0.5)\n",
        "            >>> keysents = summarizer.summarize(texts, topk=30)\n",
        "        \"\"\"\n",
        "        n_sents = len(sents)\n",
        "        if isinstance(bias, np.ndarray):\n",
        "            if bias.shape != (n_sents,):\n",
        "                raise ValueError('The shape of bias must be (n_sents,) but {}'.format(bias.shape))\n",
        "        elif bias is not None:\n",
        "            raise ValueError('The type of bias must be None or numpy.ndarray but the type is {}'.format(type(bias)))\n",
        "\n",
        "        self.train_textrank(sents, bias)\n",
        "        idxs = self.R.argsort()[-topk:]\n",
        "        keysents = [(idx, self.R[idx], sents[idx]) for idx in reversed(idxs)]\n",
        "        return keysents\n"
      ],
      "metadata": {
        "id": "DG_Nf5DK9tM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "자료 파일 삽입"
      ],
      "metadata": {
        "id": "BOmn9TOP-zXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz3J7-GDAmbk",
        "outputId": "2dca4ad9-87cb-49e6-819b-682a3dc0bafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 넣어둔 폴더로 이동\n",
        "% cd /content/drive/MyDrive/textrank실습/2020-02-019.도서자료요약_Sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aEOR9tXq5tp",
        "outputId": "b8f2fbf3-46e7-4cd2-d429-6477037cab0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/textrank실습/2020-02-019.도서자료요약_Sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "6jSN7jPrw9pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path 만들기\n",
        "dir=\"./\"\n",
        "sample_file=os.listdir(\"./\")[0]\n",
        "path=dir+\"/\"+sample_file \n",
        "#file descriptor 생성하고 내용 읽어내기\n",
        "fstr=open(path,encoding='utf-8').read()\n",
        "print(sample_file)\n",
        "#json decode 하기\n",
        "sample_json=json.loads(fstr)\n",
        "print(sample_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjom_tOYzFie",
        "outputId": "a2e2e994-c3bb-4f75-eca7-360ac622b754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNTS-00049603863_154_154-0.json\n",
            "{'passage_id': 'CNTS-00049603863_154_154-0', 'metadata': {'doc_id': 'CNTS-00049603863', 'doc_type': '도서', 'doc_name': '금융정보화 현황 및 과제', 'author': '금융정보화추진분과위원회 사무국', 'publisher': '한국은행', 'published_year': '1998', 'kdc_label': '경제학', 'kdc_code': '320'}, 'chapter': \"제 5 장 금융정보화의 과제 제 4절 '2000년 문제'에의 대응\", 'passage': '가. \"2000년 문제’의 내용\\n\\'2000년 문제\\'는 대다수 기존 컴퓨터의 H/W와 S/W가 비용절감을 위해 연도 네 자리 중 마지막 두 자리만 표기하여 인식하도록 설계됨으로써, 향후 2000년대와 1900년대를 구분 인식하지 못하여 발생하는 컴퓨터 자체의 장애나 연산오류 발생 등의 문제를 말하는데 일명 Millennium Bug(\\'천년’을 뜻하는 Millennium과 \\'컴퓨터상의 오류\\'를 나타내는 Bug의 합성어) 또는 Y2K(Y는 연도, K는 천단위 Kilo) 문제로도 불린다. \\n2000년 1월 1일부터 컴퓨터기기 자체 또는 운영체제상의 기술적인 문제로 컴퓨터의 작동이 전면 또는 일부 중단되거나 응용S/W의 결함으로 컴퓨터를 이용한 모든 업무처리가 마비되는 사태가 발생할 가능성에 철저히 대비하여야 한다. 그러나 컴퓨터시스템은 대단히 복잡하여 미처 예상치 못한 문제발생 소지가 많으므로 완전 해결여부를 사전에 판단하기는 불가능하다는 것이 일반적인 견해이다. 이는 컴퓨터시스템이 수많은 종류의 H/W 및 S/W, 그리고 방대한 네트워크로 구성되어 있어 다양한 형태의 문제를 야기시킬 소지가 있기 때문이다.\\n따라서 대응작업을 조기에 완료한 후 가능한 한 테스트 기간을 길게 확보하여 지속적인 테스트를 실시하면서 문제를 하나 하나 보완/해결해 나가는 방식이 세계적인 일반 추세이다.', 'summary': '연대를 구분하지 못하여 컴퓨터 자체의 장애, 연산오류 발생 등을  Millennium Bug나 Y2K문제라고 불린다. 2000년부터 컴퓨터 작동이 중단되거나 업무처리가 마비되는 가능성을 대비해야하나 컴퓨터 자체의 복잡성으로 인해 사전 판단하기는 어렵다. 이를 대비하는 추세는 대응작업을 마친 뒤 테스트 기간을 연장하여 실시하면서 문제를 해결하는 것이다. '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set example text\n",
        "\n",
        "#1) open json file\n",
        "## get path\n",
        "## get file descriptor\n",
        "## get str\n",
        "\n",
        "dir=\"./\"\n",
        "jsonfd=[]\n",
        "jsonstr=[]\n",
        "for file in os.listdir(dir):\n",
        "    jsonfd.append(open(dir+\"/\"+file, encoding=\"utf-8\"))\n",
        "    jsonstr.append(jsonfd[-1].read())\n",
        "\n",
        "#2) select passages and summaries\n",
        "jsonobj=[]\n",
        "count=0\n",
        "id={}\n",
        "passage={}\n",
        "summary={}\n",
        "for file in jsonstr:\n",
        "    obj=json.loads(file)\n",
        "    jsonobj.append(obj)\n",
        "    id[count]=obj[\"passage_id\"]\n",
        "    passage[count]=obj[\"passage\"]\n",
        "    summary[count]=obj[\"summary\"]"
      ],
      "metadata": {
        "id": "ylZzzvO-zbqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Length check\",len(passage)==len(summary),\"\\nFirst set\\n\",\"\\t\\tPassage\",passage[0],\"\\n\\t\\tSummary\",summary[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiHnGPKA0_ar",
        "outputId": "26f1901a-57f6-4dbc-baa3-139a44c4ac12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length check True \n",
            "First set\n",
            " \t\tPassage 팝업씨어터의 ‘피해자’로 규정된 이후, ‘지금, 여기’를 살고 있는 우리들을 가끔, 그때의 고통을 (재)증언하기 위해, 다시 ‘그때, 거기’의 피해자로 돌아가라는 요청을 받기도 한다. 블랙리스트의 끔찍함을 체감하지 못하는 관료들과 시민들, 그리고 후배들과 선배들을 위해 다시 한번 팝업씨어터의 무대에서 역사적 퍼포먼스를 펼치게 되는 것이다. 그러나 그것은 결코 영웅담이나 노스탤지어가 될 수 없다. 반복해서 재수행하면 할수록, 외려 그것과 멀어지고 싶은 마음이 들 뿐이다. 팝업씨어터 이후 연극하는 동료들이 더더욱 창작에 몰두하게 된 것은, 보란 듯이 만회하겠다는 예술가의 포부나 혹은 오기 같은 것이 아니라, 어쩌면 ‘피해자’로 머물러 있는 그 규정된 정체성에서 하루빨리 벗어나고 싶었던 것은 아닐까. \n",
            "\t\tSummary 블랙리스트로 인한 팝업씨어터의 피해자들은 피해의 증언을 위해 다시 상기하라는 요구를 받지만 이는 추억이 될 수는 없다. 그 피해자들의 노력은 피해자에서 벗어나고자하는 의지에서 나온 것이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konply를 쓰기위한 환경 구축"
      ],
      "metadata": {
        "id": "kstR1FWBvA4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "metadata": {
        "id": "iTjRZ3AJIog4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bw9lwwGRvEi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "id": "En2pXGHbIsp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef546ef4-cefd-4947-b7ca-50cf86be1013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "metadata": {
        "id": "12L560IkIuKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Kkma, Komoran\n",
        "from konlpy.utils import pprint"
      ],
      "metadata": {
        "id": "fpyphx0_IzsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "본격적인 문장 추출을 위해 도서 문장 원본 하나를 sents라는 변수에 저장하였다."
      ],
      "metadata": {
        "id": "TdL30uzXvGTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents =['선거개혁도 유사한 문제가 있다.', '유권자들은 1979년 현급 지방인대 직접선거 개혁 –제한적 경쟁선거의 도입, 유권자의 후보추천권 보장 – 이후 좀 더 적극적이고 능동적으로 선거에 참여한다.', '그래서 1990년대에 들어 직접선거는 정치교육과 사회화를 통해 국민의 정권에 대한 정통성과 수용성을 높이려는 공산당의 동원수단에서 벗어나 점차로 국민의 의지와 요구를 실현하는 수단으로 발전하였다.', '그렇지만 중국의 모든 선거는 공산당 일당지배 체제하에 진행된다는 근본적 한계 외에도, 성급 지방인대 대표와 전국인대 대표선거는 여전히 간접선거로 실시되고 있으며, 중앙정부는 말할 것도 없고 지방정부 수장들도 모두 의회에서 간접선거로 선출된다는 문제점이 있다.', '이 때문에 선거를 통한 국민의 정치참여 확대도 전보다 나아지기는 했지만 여전히 많은 한계를 갖는다.']"
      ],
      "metadata": {
        "id": "DgObtqF2sqe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WmP_QniXvQ0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = KeysentenceSummarizer(\n",
        "    tokenize = lambda x:x.split(),\n",
        "    min_sim = 0.3,\n",
        "    verbose = False\n",
        ")\n",
        "keysents = summarizer.summarize(sents, topk=2)\n",
        "for _, _, sent in keysents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "id": "qsWoDvnpF9YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "summarizer 의 R 에는 각 문장 별 중요도 (PageRank 값) 가 저장되어 있습니다."
      ],
      "metadata": {
        "id": "9ZnhjYgAIRYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxnSfwatISQ3",
        "outputId": "2f34da56-b005-4158-e46e-7061f2777b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.15      , 0.15      , 0.79469195, 0.90454963, 1.30075842])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장의 위치에 따라 중요도를 다르게 설정할 수도 있습니다. 뉴스 기사는 대부분 첫 문장이 중요합니다. 실제로 위의 예시에서도 첫 문장이 가장 중요한 핵심 문장으로 선택되었습니다. 만약 마지막 문장이 중요하다고 가정한다면 이러한 정보를 bias 에 추가할 수 있습니다. numpy.ndarray 형태로 bias 를 만듭니다. 마지막 문장이 다른 문장보다 10 배 중요하다고 가정하였습니다. 이를 summarize 함수의 bias 에 입력하면 가장 먼저 맨 마지막 문장이 중요한 문장으로 선택됩니다. 다른 문장들 중에서도 맨 마지막 문장과 비슷할수록 상대적인 중요도가 더 커집니다."
      ],
      "metadata": {
        "id": "P2ZSMq8PIXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "bias = np.ones(len(sents))\n",
        "bias[-1] = 10\n",
        "\n",
        "keysents = summarizer.summarize(sents, topk=3, bias=bias)\n",
        "for _, _, sent in keysents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCPHnf4xIYQ0",
        "outputId": "d4ba1e96-92bc-4282-bb81-6d1a9ce1cd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이 때문에 선거를 통한 국민의 정치참여 확대도 전보다 나아지기는 했지만 여전히 많은 한계를 갖는다.\n",
            "그래서 1990년대에 들어 직접선거는 정치교육과 사회화를 통해 국민의 정권에 대한 정통성과 수용성을 높이려는 공산당의 동원수단에서 벗어나 점차로 국민의 의지와 요구를 실현하는 수단으로 발전하였다.\n",
            "그렇지만 중국의 모든 선거는 공산당 일당지배 체제하에 진행된다는 근본적 한계 외에도, 성급 지방인대 대표와 전국인대 대표선거는 여전히 간접선거로 실시되고 있으며, 중앙정부는 말할 것도 없고 지방정부 수장들도 모두 의회에서 간접선거로 선출된다는 문제점이 있다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R 을 다시 확인해보면 PageRank 값이 달라졌음을 확인할 수 있습니다. 상대적인 위치 외에도 특정 단어가 포함된 문장에 preference (bias) 를 더 높게 설정할 수도 있습니다."
      ],
      "metadata": {
        "id": "XxMzynziIbz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.R"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1powcbIxIfYU",
        "outputId": "aa3ee58d-2bbe-46fc-885b-10135de0c738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05357143, 0.05357143, 1.01654242, 1.0078116 , 2.25154929])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}