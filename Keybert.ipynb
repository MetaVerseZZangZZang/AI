{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a668e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.12.1-cp38-cp38-win_amd64.whl (161.9 MB)\n",
      "     ------------------------------------- 161.9/161.9 MB 29.8 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.13.1-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 70.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence_transformers) (1.23.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp38-cp38-win_amd64.whl (7.3 MB)\n",
      "     ---------------------------------------- 7.3/7.3 MB 93.9 MB/s eta 0:00:00\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.9.2-cp38-cp38-win_amd64.whl (39.8 MB)\n",
      "     --------------------------------------- 39.8/39.8 MB 65.5 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 70.2 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "     ---------------------------------------- 155.4/155.4 kB ? eta 0:00:00\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 69.8 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.9.13-cp38-cp38-win_amd64.whl (267 kB)\n",
      "     ---------------------------------------- 267.7/267.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 71.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=94aec8e47b388e8fcc4e17096a4cd6b6b873ab3f2ef4d83ede1522c845760946\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\5e\\6f\\8c\\d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, tqdm, torch, threadpoolctl, scipy, regex, pyyaml, pillow, joblib, filelock, click, torchvision, scikit-learn, nltk, huggingface-hub, transformers, sentence_transformers\n",
      "Successfully installed click-8.1.3 filelock-3.8.0 huggingface-hub-0.10.1 joblib-1.2.0 nltk-3.7 pillow-9.2.0 pyyaml-6.0 regex-2022.9.13 scikit-learn-1.1.2 scipy-1.9.2 sentence_transformers-2.2.2 sentencepiece-0.1.97 threadpoolctl-3.1.0 tokenizers-0.13.1 torch-1.12.1 torchvision-0.13.1 tqdm-4.64.1 transformers-4.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8e229c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\users\\hp\\miniconda3\\envs\\project\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\hp\\miniconda3\\envs\\project\\lib\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\hp\\miniconda3\\envs\\project\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\hp\\miniconda3\\envs\\project\\lib\\site-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\miniconda3\\envs\\project\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660c6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from konlpy.tag import Mecab\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bee885b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '날씨']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab = Mecab(dicpath=r'C:\\mecab\\mecab-ko-dic')\n",
    "out = mecab.pos(phrase = \"오늘은 맑은 날씨이다.\")\n",
    "[word[0] for word in out if word[1] == 'NNG' or word[1] == 'NNP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6de3ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "\"박홍근 위원] \\\"그러면 지금 마지막 남은 여섯 번째 감시시편 있지 않습니까? 이 부분은 언제 꺼내서 할 계획으로 돼 있습니까?\\\"\\n한국원자력안전기술원장 박윤원] \\\"그것은 저희가 꺼내서 하는 것은 아니고요. 아까 아마 그 지적하신 것들이 말씀하신 것들이 ‘2017년 이후에 만약에 목표를 한다면 그것은 연장을 목표로 하는 것 아니냐’하고 말씀하셨지 않습니까? 그래서 아마 미국에서 저희가 그 규정을 미국 것을 그냥 따 와서 하다 보니까 그렇게 됐는데 그것 자체가 보면 미국 자체가 60～80년 이렇게까지를 감안을 해 가지고 고려를 하고 있는 겁니다.   그런데 우리나라에서도 사실은 보면 2017년까지에 해당되는 건전성은 이미 평가가 끝났고 그 이후에 만약에 필요하다면 그러니까 계속운전을 하느냐 마나냐는 2017년 이전 최소한 2～5년 전에 결정이 돼야 되겠지요. 그것은 또 한수원이 결정해야 될 문제라고 봅니다마는 그렇게 해서 만약에 간다면 그때 그 시편을 또 사용할 수 있을 거라고 생각은 합니다.\\\"\\n박홍근 위원] \\\"그러니까 아직은 구체적인 시기가 결정된 바는 없다 이거지요?\\\"\\n한국원자력안전기술원장 박윤원] \\\"아직은 없습니다.\\\"\\n박홍근 위원] \\\"그러니까 2017년 이전에 2～3년 전에 기본적으로 해야 한다 이런 말씀이시잖아요?   그런데 말씀드리는 것은 이렇습니다. 2005년도의 이 결과가 2013년도에는 기준을 일단은 지금 초과하는 것으로 나타났잖아요? 그러면 2013년과 말씀하신 그 특정 이후의 추출 꺼내는 시점 있지 않습니까? 그 공백의 사이에 이 용기 안에서의 그런 어떤 문제점에 대해서는 누가 그것을 책임질 수 있나요?\\\"\\n\",\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3cfb0f4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out = mecab.pos(doc)\n",
    "noun_list = [word[0] for word in out if word[1] == 'NNG' or word[1] == 'NNP']\n",
    "noun_list = [noun for noun in noun_list if len(noun) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dbc13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# okt = Okt()\n",
    "\n",
    "# tokenized_doc = okt.pos(doc, stem=True)\n",
    "# tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "12faee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['위원장', '위원', '선배', '저', '우리', '위', '가지', '안녕하십니까', '여러', '위해', '못', '것', '사실', '말씀', '이후', '만약', '원장', '이전', '마지막', '최소한', '생각', '사이']\n",
    "stop_names = ['강창순', '유기홍', '박성호', '유성엽', '유은혜', '정진후', '박홍근', '박윤원']\n",
    "\n",
    "stop_list = stop_words + stop_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "de6a1cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for w in noun_list:\n",
    "    if w not in stop_list:\n",
    "        result.append(w)\n",
    "        \n",
    "tokenized_nouns = ' '.join(result)\n",
    "\n",
    "# 중복제거\n",
    "unique_token_koms_noun = set(result)\n",
    "unique_noun_list = ' '.join(unique_token_koms_noun)\n",
    "\n",
    "# 명사 빈도 카운트\n",
    "count = Counter(result)\n",
    "\n",
    "common_list = count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f1920ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안전 초과 미국 감안 사용 고려 문제 나라 원자력 해당 시기 기술 공백 규정 평가 문제점 계획 한국 시편 목표 지적 수원 기준 기본 자체 추출 그때 필요 구체 연장 결정 시점 결과 부분 운전 특정 용기 감시'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_nouns = unique_noun_list\n",
    "tokenized_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c775465a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('미국', 3),\n",
       " ('결정', 3),\n",
       " ('시편', 2),\n",
       " ('한국', 2),\n",
       " ('원자력', 2),\n",
       " ('안전', 2),\n",
       " ('기술', 2),\n",
       " ('목표', 2),\n",
       " ('자체', 2),\n",
       " ('감시', 1)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4f4f741d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram 개수 : 73\n",
      "trigram 다섯개만 출력 : ['감안 사용' '감안 사용 고려' '결과 부분' '결과 부분 운전' '결정 시점']\n"
     ]
    }
   ],
   "source": [
    "n_gram_range = (2, 3)\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "candidates = count.get_feature_names_out()\n",
    "\n",
    "print('trigram 개수 :',len(candidates))\n",
    "print('trigram 다섯개만 출력 :',candidates[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e03a57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0ae783cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT로 키워드를 수치화\n",
    "#model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "051ddce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embedding = model.encode([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3a67c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_embeddings = model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "137f9b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['결과 부분 운전', '미국 감안', '문제점 계획 한국', '초과 미국 감안', '계획 한국']\n"
     ]
    }
   ],
   "source": [
    "# 가장 유사한 키워드를 추출합니다\n",
    "# 상위 5개\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be4afaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비슷한 키워드가 리턴되는 이유는 문서를 가장 잘 나타내고 있으므로\n",
    "# 따라서 좀 다양한 키워드를 원한다면 아래의 알고리즘 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2f3f527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Sum Similarity\n",
    "# 유사성을 최소화하면서 문서와의 유사성은 극대화\n",
    "\n",
    "def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):\n",
    "    # 문서와 각 키워드들 간의 유사도\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "869d6866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['한국 시편 목표', '문제 나라 원자력', '부분 운전', '초과 미국 감안', '계획 한국']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nr_candidates가 낮으면 유사한 키워드들\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fc50c924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['해당 시기', '문제점 계획', '결과 부분', '기본 자체', '한국 시편 목표']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nr_candidates가 높으면 다양한 키워드들 (30 넘어가면 시간이 너무 오래 걸림)\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "abdaa9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance 알고리즘\n",
    "# 중복을 최소화하고 결과의 다양성을 극대화\n",
    "\n",
    "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
    "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
    "\n",
    "    # 각 키워드들 간의 유사도\n",
    "    word_similarity = cosine_similarity(candidate_embeddings)\n",
    "\n",
    "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # keywords_idx = [2]\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "\n",
    "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
    "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
    "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
    "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
    "    for _ in range(top_n - 1):\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # MMR을 계산\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # keywords & candidates를 업데이트\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "04245904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계획 한국', '결과 부분 운전', '문제점 계획 한국', '초과 미국 감안', '고려 문제 나라']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diversity가 낮으면 유사한 단어들\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ec15446d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계획 한국', '공백 규정 평가', '필요 구체', '수원 기준 기본', '결과 부분 운전']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diversity가 높으면 다양한 키워드\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad5eb74c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Using cached keybert-0.6.0-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from keybert) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from keybert) (1.23.4)\n",
      "Collecting rich>=10.4.0\n",
      "  Using cached rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from keybert) (2.2.2)\n",
      "Collecting commonmark<0.10.0,>=0.9.0\n",
      "  Using cached commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from rich>=10.4.0->keybert) (4.3.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from rich>=10.4.0->keybert) (2.11.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.9.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.23.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.97)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.13.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (1.12.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.9.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (9.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\miniconda3\\envs\\mecab\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.4)\n",
      "Installing collected packages: commonmark, rich, keybert\n",
      "Successfully installed commonmark-0.9.1 keybert-0.6.0 rich-12.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6c91dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "from keybert import KeyBERT\n",
    "\n",
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model = KeyBERT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "94cf4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"\"\"\n",
    "\"박홍근 위원] \\\"그러면 지금 마지막 남은 여섯 번째 감시시편 있지 않습니까? 이 부분은 언제 꺼내서 할 계획으로 돼 있습니까?\\\"\\n한국원자력안전기술원장 박윤원] \\\"그것은 저희가 꺼내서 하는 것은 아니고요. 아까 아마 그 지적하신 것들이 말씀하신 것들이 ‘2017년 이후에 만약에 목표를 한다면 그것은 연장을 목표로 하는 것 아니냐’하고 말씀하셨지 않습니까? 그래서 아마 미국에서 저희가 그 규정을 미국 것을 그냥 따 와서 하다 보니까 그렇게 됐는데 그것 자체가 보면 미국 자체가 60～80년 이렇게까지를 감안을 해 가지고 고려를 하고 있는 겁니다.   그런데 우리나라에서도 사실은 보면 2017년까지에 해당되는 건전성은 이미 평가가 끝났고 그 이후에 만약에 필요하다면 그러니까 계속운전을 하느냐 마나냐는 2017년 이전 최소한 2～5년 전에 결정이 돼야 되겠지요. 그것은 또 한수원이 결정해야 될 문제라고 봅니다마는 그렇게 해서 만약에 간다면 그때 그 시편을 또 사용할 수 있을 거라고 생각은 합니다.\\\"\\n박홍근 위원] \\\"그러니까 아직은 구체적인 시기가 결정된 바는 없다 이거지요?\\\"\\n한국원자력안전기술원장 박윤원] \\\"아직은 없습니다.\\\"\\n박홍근 위원] \\\"그러니까 2017년 이전에 2～3년 전에 기본적으로 해야 한다 이런 말씀이시잖아요?   그런데 말씀드리는 것은 이렇습니다. 2005년도의 이 결과가 2013년도에는 기준을 일단은 지금 초과하는 것으로 나타났잖아요? 그러면 2013년과 말씀하신 그 특정 이후의 추출 꺼내는 시점 있지 않습니까? 그 공백의 사이에 이 용기 안에서의 그런 어떤 문제점에 대해서는 누가 그것을 책임질 수 있나요?\\\"\\n\",\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03411b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"박홍근 위원] \"그러면 지금 마지막 남은 여섯 번째 감시시편 있지 않습니까? 이 부분은 언제 꺼내서 할 계획으로 돼 있습니까?\"\\n한국원자력안전기술원장 박윤원] \"그것은 저희가 꺼내서 하는 것은 아니고요. 아까 아마 그 지적하신 것들이 말씀하신 것들이 ‘2017년 이후에 만약에 목표를 한다면 그것은 연장을 목표로 하는 것 아니냐’하고 말씀하셨지 않습니까? 그래서 아마 미국에서 저희가 그 규정을 미국 것을 그냥 따 와서 하다 보니까 그렇게 됐는데 그것 자체가 보면 미국 자체가 60～80년 이렇게까지를 감안을 해 가지고 고려를 하고 있는 겁니다.   그런데 우리나라에서도 사실은 보면 2017년까지에 해당되는 건전성은 이미 평가가 끝났고 그 이후에 만약에 필요하다면 그러니까 계속운전을 하느냐 마나냐는 2017년 이전 최소한 2～5년 전에 결정이 돼야 되겠지요. 그것은 또 한수원이 결정해야 될 문제라고 봅니다마는 그렇게 해서 만약에 간다면 그때 그 시편을 또 사용할 수 있을 거라고 생각은 합니다.\"\\n박홍근 위원] \"그러니까 아직은 구체적인 시기가 결정된 바는 없다 이거지요?\"\\n한국원자력안전기술원장 박윤원] \"아직은 없습니다.\"\\n박홍근 위원] \"그러니까 2017년 이전에 2～3년 전에 기본적으로 해야 한다 이런 말씀이시잖아요?   그런데 말씀드리는 것은 이렇습니다. 2005년도의 이 결과가 2013년도에는 기준을 일단은 지금 초과하는 것으로 나타났잖아요? 그러면 2013년과 말씀하신 그 특정 이후의 추출 꺼내는 시점 있지 않습니까? 그 공백의 사이에 이 용기 안에서의 그런 어떤 문제점에 대해서는 누가 그것을 책임질 수 있나요?\"\\n\",\\n      '"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "74aba4d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['박홍근',\n",
       " '위원',\n",
       " '마지막',\n",
       " '감시',\n",
       " '시편',\n",
       " '부분',\n",
       " '계획',\n",
       " '한국',\n",
       " '원자력',\n",
       " '안전',\n",
       " '기술',\n",
       " '원장',\n",
       " '박윤원',\n",
       " '지적',\n",
       " '말씀',\n",
       " '이후',\n",
       " '만약',\n",
       " '목표',\n",
       " '연장',\n",
       " '목표',\n",
       " '말씀',\n",
       " '미국',\n",
       " '규정',\n",
       " '미국',\n",
       " '자체',\n",
       " '미국',\n",
       " '자체',\n",
       " '감안',\n",
       " '고려',\n",
       " '나라',\n",
       " '해당',\n",
       " '평가',\n",
       " '이후',\n",
       " '만약',\n",
       " '필요',\n",
       " '운전',\n",
       " '이전',\n",
       " '최소한',\n",
       " '결정',\n",
       " '수원',\n",
       " '결정',\n",
       " '문제',\n",
       " '만약',\n",
       " '그때',\n",
       " '시편',\n",
       " '사용',\n",
       " '생각',\n",
       " '박홍근',\n",
       " '위원',\n",
       " '구체',\n",
       " '시기',\n",
       " '결정',\n",
       " '한국',\n",
       " '원자력',\n",
       " '안전',\n",
       " '기술',\n",
       " '원장',\n",
       " '박윤원',\n",
       " '박홍근',\n",
       " '위원',\n",
       " '이전',\n",
       " '기본',\n",
       " '말씀',\n",
       " '말씀',\n",
       " '결과',\n",
       " '기준',\n",
       " '초과',\n",
       " '말씀',\n",
       " '특정',\n",
       " '이후',\n",
       " '추출',\n",
       " '시점',\n",
       " '공백',\n",
       " '사이',\n",
       " '용기',\n",
       " '문제점']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mecab.pos(doc2)\n",
    "noun_list = [word[0] for word in out if word[1] == 'NNG' or word[1] == 'NNP']\n",
    "noun_list = [noun for noun in noun_list if len(noun) > 1]\n",
    "noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "46bb61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for w in noun_list:\n",
    "    if w not in stop_list:\n",
    "        result.append(w)\n",
    "        \n",
    "tokenized_nouns = ' '.join(result)\n",
    "\n",
    "# 중복제거\n",
    "unique_token_koms_noun = set(result)\n",
    "unique_noun_list = ' '.join(unique_token_koms_noun)\n",
    "\n",
    "# 명사 빈도 카운트\n",
    "count = Counter(result)\n",
    "\n",
    "common_list = count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9945bca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('미국', 3),\n",
       " ('결정', 3),\n",
       " ('시편', 2),\n",
       " ('한국', 2),\n",
       " ('원자력', 2),\n",
       " ('안전', 2),\n",
       " ('기술', 2),\n",
       " ('목표', 2),\n",
       " ('자체', 2),\n",
       " ('감시', 1)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7ccb9a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안전 초과 미국 감안 사용 고려 문제 나라 원자력 해당 시기 기술 공백 규정 평가 문제점 계획 한국 시편 목표 지적 수원 기준 기본 자체 추출 그때 필요 구체 연장 결정 시점 결과 부분 운전 특정 용기 감시'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_nouns = unique_noun_list\n",
    "tokenized_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8c593a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('원자력', 0.5887),\n",
       " ('안전', 0.5808),\n",
       " ('감시', 0.5639),\n",
       " ('규정', 0.4851),\n",
       " ('한국', 0.4628)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(tokenized_nouns, keyphrase_ngram_range=(1, 1), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5ffeed99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나라 원자력', 0.6155),\n",
       " ('안전 초과', 0.6067),\n",
       " ('원자력 해당', 0.594),\n",
       " ('기술 공백', 0.5337),\n",
       " ('규정 평가', 0.5295)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(tokenized_nouns, keyphrase_ngram_range=(2, 2), stop_words=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9eb8b81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('운전 특정', 0.4178),\n",
       " ('한국', 0.4628),\n",
       " ('규정 평가', 0.5295),\n",
       " ('기술 공백', 0.5337),\n",
       " ('나라 원자력', 0.6155)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_model.extract_keywords(tokenized_nouns, keyphrase_ngram_range=(1,2), stop_words='english',\n",
    "                              use_maxsum=True, nr_candidates=30, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6ef5d68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('나라 원자력', 0.6155),\n",
       " ('기술 공백', 0.5337),\n",
       " ('규정 평가', 0.5295),\n",
       " ('한국 시편', 0.3419),\n",
       " ('운전', 0.2757)]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다양\n",
    "kw_model.extract_keywords(tokenized_nouns, keyphrase_ngram_range=(1,2), stop_words='english',\n",
    "                              use_mmr=True, diversity=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312d81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b39fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fd12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faae48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaae453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11191df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23c9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
